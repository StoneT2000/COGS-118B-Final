{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4baa0cd-01bb-4961-8778-f05c8099a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77877024-1c87-44f7-ac17-5396b1adaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "dataset[\"train\"] = dataset[\"train\"] / 255\n",
    "dataset[\"train\"] = dataset[\"train\"].permute(0,3,1,2)\n",
    "dataset[\"train\"].shape\n",
    "data_path = \"../../tiny-imagenet-200\"\n",
    "classes = pd.read_csv(f\"{data_path}/words.txt\", sep=\"\\t\", names=[\"id\", \"entity\"])\n",
    "id_to_label = {}\n",
    "for _, row in classes.iterrows():\n",
    "    id_to_label[row['id']] = row['entity']\n",
    "labels = np.array(dataset['labels'])\n",
    "label_to_idx = {}\n",
    "for i, label in enumerate(np.unique(labels)):\n",
    "    label_to_idx[label] = i\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = label_to_idx[labels[i]]\n",
    "labels = labels.astype(int)\n",
    "labels = torch.from_numpy(labels)\n",
    "# labels_onehot = torch.nn.functional.one_hot(labels,)\n",
    "raw_dataset = []\n",
    "for i in range(len(labels_onehot)):\n",
    "    raw_dataset.append((dataset[\"train\"][i], labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a879d-b97e-4cec-b595-9ce00471aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accs(preds, labels):\n",
    "    k = 10\n",
    "    _, ind = torch.topk(preds, k)\n",
    "    \n",
    "    accs = {}\n",
    "    for _k in [1, 5, 10]:\n",
    "        acc = ind[:,:_k].eq(labels).any(1).sum() / len(labels)\n",
    "        accs[f\"top_{_k}_acc\"] = acc\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43ec6c-2e38-4484-bd05-8c5ec62ed6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_nb\n",
    "def evaluate(model, val_dl: DataLoader, verbose=True, nb=False):\n",
    "    loss_fnc = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    _tqdm = tqdm_nb if nb else tqdm\n",
    "    if verbose:\n",
    "        pbar = _tqdm(enumerate(val_dl), total=len(val_dl), position=0, leave=True)\n",
    "        pbar.set_description(\"Evaluation Progress\")\n",
    "    with torch.no_grad():\n",
    "        batch_size = val_dl.batch_size\n",
    "        avg_loss = defaultdict(int)\n",
    "        num_train_imgs = len(val_dl.dataset)\n",
    "        c = 0\n",
    "        for batch_idx, data in enumerate(val_dl):\n",
    "            c += 1\n",
    "\n",
    "            imgs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            preds = model(imgs)\n",
    "            preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "\n",
    "            loss = {\n",
    "                \"loss\": loss_fnc(preds, labels),\n",
    "                # \"acc\": 0\n",
    "            }\n",
    "            for k, v in compute_accs(preds, labels.view(-1,1)).items():\n",
    "                loss[k] = v\n",
    "            for k, v in loss.items():\n",
    "                avg_loss[k] += v.item()\n",
    "            if verbose:\n",
    "                pbar.update()\n",
    "        for k in avg_loss.keys():\n",
    "            avg_loss[k] /= c\n",
    "    model.train()\n",
    "    if verbose:\n",
    "        pbar.close()\n",
    "    return avg_loss\n",
    "def train(\n",
    "    model,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    train_dl: DataLoader,\n",
    "    val_dl: DataLoader,\n",
    "    start_epoch=0,\n",
    "    save_freq=10,\n",
    "    save_best=True,\n",
    "    save_dir=\"./\",\n",
    "    prev_best_loss=np.inf,\n",
    "    verbose=True,\n",
    "    nb=False,\n",
    "    train_cb=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    train the vae model\n",
    "    \"\"\"\n",
    "    loss_fnc = torch.nn.CrossEntropyLoss()\n",
    "    _tqdm = tqdm_nb if nb else tqdm\n",
    "    model.train()\n",
    "    optimizer_idx = 0\n",
    "    num_train_imgs = len(train_dl.dataset)\n",
    "    batch_size = train_dl.batch_size\n",
    "    if verbose:\n",
    "        epoch_pbar = _tqdm(range(epochs), position=0, leave=True)\n",
    "        epoch_pbar.set_description(\"Progress\")\n",
    "        pbar = _tqdm(enumerate(train_dl), total=len(train_dl), position=0, leave=True)\n",
    "        pbar.set_description(\"Current Epoch Progress\")\n",
    "        epoch_pbar.update(start_epoch)\n",
    "    for epoch in range(start_epoch, start_epoch + epochs):\n",
    "        avg_loss = defaultdict(int)\n",
    "        c = 0\n",
    "        if verbose:\n",
    "            epoch_pbar.update()\n",
    "            pbar.reset()\n",
    "        for batch_idx, data in enumerate(train_dl):\n",
    "\n",
    "            c += 1\n",
    "            \n",
    "            imgs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            # imgs (B, C=1, W, H)\n",
    "            optim.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "            loss = {\n",
    "                \"loss\": loss_fnc(preds, labels),\n",
    "                # \"acc\": \n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                for k, v in compute_accs(preds, labels.view(-1,1)).items():\n",
    "                    loss[k] = v\n",
    "                for k, v in loss.items():\n",
    "                    avg_loss[k] += v.item()\n",
    "            optimizer_idx += 1\n",
    "            loss[\"loss\"].backward()\n",
    "            optim.step()\n",
    "            if verbose:\n",
    "                pbar.update()\n",
    "        for k in avg_loss.keys():\n",
    "            avg_loss[k] /= c\n",
    "        eval_loss = evaluate(model, val_dl, nb=nb, verbose=False)\n",
    "        if train_cb:\n",
    "            train_cb(epoch=epoch, loss=avg_loss, eval_loss=eval_loss, model=model)\n",
    "        save = False\n",
    "        if eval_loss[\"loss\"] < prev_best_loss:\n",
    "            prev_best_loss = eval_loss[\"loss\"]\n",
    "            if save_best:\n",
    "                save = True\n",
    "        if epoch % save_freq == 0 or epoch == epochs - 1:\n",
    "            save = True\n",
    "        if save:\n",
    "            torch.save(\n",
    "                dict(\n",
    "                    model_state_dict=model.state_dict(),\n",
    "                    optim_State_dict=optim.state_dict(),\n",
    "                    epoch=epoch,\n",
    "                    loss=avg_loss,\n",
    "                    eval_loss=eval_loss,\n",
    "                    prev_best_loss=prev_best_loss,\n",
    "                ),\n",
    "                osp.join(save_dir, f\"ckpt_{epoch}.pt\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c12240-e1d1-4506-86ce-99d4c91a2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "shuffled_raw_dataset = shuffle(raw_dataset, random_state=3407)\n",
    "train_raw_dataset = shuffled_raw_dataset[:60000]\n",
    "val_raw_dataset = shuffled_raw_dataset[60000:60000+20000]\n",
    "test_raw_dataset = shuffled_raw_dataset[60000+20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir alexnet_ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531c09c-0fa0-4dec-8b8f-e51e4d20f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3407)\n",
    "batch_size=256\n",
    "train_dl = torch.utils.data.DataLoader(train_raw_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_dl = torch.utils.data.DataLoader(val_raw_dataset, shuffle=True, batch_size=batch_size)\n",
    "mobilenet_v2 = models.alexnet(pretrained=False, num_classes=200).to(device)\n",
    "optim = torch.optim.Adam(mobilenet_v2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382c3958-1ce5-4e3c-9ed0-1117bcb70d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cb(epoch, loss, eval_loss,model):\n",
    "    print(f\"Epoch={epoch}, loss={loss}, eval_loss={eval_loss}\")\n",
    "train(model=mobilenet_v2, optim=optim, epochs=10, train_dl=train_dl, val_dl=train_dl, train_cb=train_cb, save_dir=\"alexnet_ckpts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "31d2bc53-92d3-4d0e-8840-71b72d08462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([148,   8,   2,   3,   4])\n",
      "tensor(186)\n",
      "torch.Size([16, 5]) torch.Size([16])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/4grjffpn5wd1jpjknw1c79_80000gn/T/ipykernel_35096/3907569554.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a = torch.nn.functional.softmax(a)\n"
     ]
    }
   ],
   "source": [
    "for p in train_dl:\n",
    "    a = mobilenet_v2(p[0])\n",
    "    a = torch.nn.functional.softmax(a, dim=1)\n",
    "    # k=5\n",
    "    # ind = np.argpartition(a, -k, 1)[-k:]\n",
    "    # # print(a.argmax(1), p[1].argmax(1))\n",
    "    # print(ind.shape)\n",
    "    k=5\n",
    "    v, ind = torch.topk(a, k)\n",
    "    print(ind[0])\n",
    "    # print(p[1].argmax(1))\n",
    "    print(p[1][0].argmax())\n",
    "    print(ind.shape, p[1].argmax(1).shape)\n",
    "    print(ind.eq(p[1].argmax(1).view(-1, 1)).any(1))\n",
    "    \n",
    "    # print(ind, a[0][ind])\n",
    "    # print(a[0][ind[:,0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c0a3d-a0df-49b8-a642-51998e67934b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
