{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4baa0cd-01bb-4961-8778-f05c8099a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77877024-1c87-44f7-ac17-5396b1adaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "dataset[\"train\"] = dataset[\"train\"] / 255\n",
    "dataset[\"train\"] = dataset[\"train\"].permute(0,3,1,2)\n",
    "dataset[\"train\"].shape\n",
    "data_path = \"../../tiny-imagenet-200\"\n",
    "classes = pd.read_csv(f\"{data_path}/words.txt\", sep=\"\\t\", names=[\"id\", \"entity\"])\n",
    "id_to_label = {}\n",
    "for _, row in classes.iterrows():\n",
    "    id_to_label[row['id']] = row['entity']\n",
    "labels = np.array(dataset['labels'])\n",
    "label_to_idx = {}\n",
    "for i, label in enumerate(np.unique(labels)):\n",
    "    label_to_idx[label] = i\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = label_to_idx[labels[i]]\n",
    "labels = labels.astype(int)\n",
    "labels = torch.from_numpy(labels)\n",
    "raw_dataset = []\n",
    "for i in range(len(labels)):\n",
    "    raw_dataset.append((dataset[\"train\"][i], labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2a879d-b97e-4cec-b595-9ce00471aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accs(preds, labels):\n",
    "    k = 10\n",
    "    _, ind = torch.topk(preds, k)\n",
    "    \n",
    "    accs = {}\n",
    "    for _k in [1, 5, 10]:\n",
    "        acc = ind[:,:_k].eq(labels).any(1).sum() / len(labels)\n",
    "        accs[f\"top_{_k}_acc\"] = acc\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8d43ec6c-2e38-4484-bd05-8c5ec62ed6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_nb\n",
    "def evaluate(model, val_dl: DataLoader, verbose=True, nb=False):\n",
    "    loss_fnc = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    _tqdm = tqdm_nb if nb else tqdm\n",
    "    if verbose:\n",
    "        pbar = _tqdm(enumerate(val_dl), total=len(val_dl), position=0, leave=True)\n",
    "        pbar.set_description(\"Evaluation Progress\")\n",
    "    with torch.no_grad():\n",
    "        batch_size = val_dl.batch_size\n",
    "        avg_loss = defaultdict(int)\n",
    "        num_train_imgs = len(val_dl.dataset)\n",
    "        c = 0\n",
    "        for batch_idx, data in enumerate(val_dl):\n",
    "            c += 1\n",
    "\n",
    "            imgs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            preds = model(imgs)\n",
    "#             preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "\n",
    "            loss = {\n",
    "                \"loss\": loss_fnc(preds, labels),\n",
    "                # \"acc\": 0\n",
    "            }\n",
    "            for k, v in compute_accs(preds, labels.view(-1,1)).items():\n",
    "                loss[k] = v\n",
    "            for k, v in loss.items():\n",
    "                avg_loss[k] += v.item()\n",
    "            if verbose:\n",
    "                pbar.update()\n",
    "        for k in avg_loss.keys():\n",
    "            avg_loss[k] /= c\n",
    "    model.train()\n",
    "    if verbose:\n",
    "        pbar.close()\n",
    "    return avg_loss\n",
    "def train(\n",
    "    model,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    train_dl: DataLoader,\n",
    "    val_dl: DataLoader,\n",
    "    start_epoch=0,\n",
    "    save_freq=10,\n",
    "    save_best=True,\n",
    "    save_dir=\"./\",\n",
    "    prev_best_loss=np.inf,\n",
    "    verbose=True,\n",
    "    nb=False,\n",
    "    train_cb=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    train the vae model\n",
    "    \"\"\"\n",
    "    loss_fnc = torch.nn.CrossEntropyLoss()\n",
    "    _tqdm = tqdm_nb if nb else tqdm\n",
    "    model.train()\n",
    "    optimizer_idx = 0\n",
    "    num_train_imgs = len(train_dl.dataset)\n",
    "    batch_size = train_dl.batch_size\n",
    "    if verbose:\n",
    "        epoch_pbar = _tqdm(range(epochs), position=0, leave=True)\n",
    "        epoch_pbar.set_description(\"Progress\")\n",
    "        pbar = _tqdm(enumerate(train_dl), total=len(train_dl), position=0, leave=True)\n",
    "        pbar.set_description(\"Current Epoch Progress\")\n",
    "        epoch_pbar.update(start_epoch)\n",
    "    for epoch in range(start_epoch, start_epoch + epochs):\n",
    "        avg_loss = defaultdict(int)\n",
    "        c = 0\n",
    "        if verbose:\n",
    "            epoch_pbar.update()\n",
    "            pbar.reset()\n",
    "        for batch_idx, data in enumerate(train_dl):\n",
    "\n",
    "            c += 1\n",
    "            \n",
    "            imgs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            # imgs (B, C=1, W, H)\n",
    "            optim.zero_grad()\n",
    "            preds = model(imgs)\n",
    "#             preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "            loss = {\n",
    "                \"loss\": loss_fnc(preds, labels),\n",
    "                # \"acc\": \n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                for k, v in compute_accs(preds, labels.view(-1,1)).items():\n",
    "                    loss[k] = v\n",
    "                for k, v in loss.items():\n",
    "                    avg_loss[k] += v.item()\n",
    "            optimizer_idx += 1\n",
    "            loss[\"loss\"].backward()\n",
    "            optim.step()\n",
    "            if verbose:\n",
    "                pbar.update()\n",
    "        for k in avg_loss.keys():\n",
    "            avg_loss[k] /= c\n",
    "        eval_loss = evaluate(model, val_dl, nb=nb, verbose=False)\n",
    "        if train_cb:\n",
    "            train_cb(epoch=epoch, loss=avg_loss, eval_loss=eval_loss, model=model)\n",
    "        save = False\n",
    "        if eval_loss[\"loss\"] < prev_best_loss:\n",
    "            prev_best_loss = eval_loss[\"loss\"]\n",
    "            if save_best:\n",
    "                save = True\n",
    "        if epoch % save_freq == 0 or epoch == epochs - 1:\n",
    "            save = True\n",
    "        torch.save(\n",
    "            dict(\n",
    "                epoch=epoch,\n",
    "                loss=avg_loss,\n",
    "                eval_loss=eval_loss,\n",
    "                prev_best_loss=prev_best_loss,\n",
    "            ),\n",
    "            osp.join(save_dir, f\"history/ckpt_{epoch}.pt\"),\n",
    "        )\n",
    "        if save:\n",
    "            torch.save(\n",
    "                dict(\n",
    "                    model_state_dict=model.state_dict(),\n",
    "                    optim_State_dict=optim.state_dict(),\n",
    "                    epoch=epoch,\n",
    "                    loss=avg_loss,\n",
    "                    eval_loss=eval_loss,\n",
    "                    prev_best_loss=prev_best_loss,\n",
    "                ),\n",
    "                osp.join(save_dir, f\"weights/ckpt_{epoch}.pt\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b9c12240-e1d1-4506-86ce-99d4c91a2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "shuffled_raw_dataset = shuffle(raw_dataset, random_state=3407)\n",
    "train_raw_dataset = shuffled_raw_dataset[:60000]\n",
    "val_raw_dataset = shuffled_raw_dataset[60000:60000+20000]\n",
    "test_raw_dataset = shuffled_raw_dataset[60000+20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61b9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS\n",
    "!mkdir resnet18_ckpts\n",
    "!mkdir resnet18_ckpts/history\n",
    "!mkdir resnet18_ckpts/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8531c09c-0fa0-4dec-8b8f-e51e4d20f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(340)\n",
    "batch_size=256\n",
    "train_dl = torch.utils.data.DataLoader(train_raw_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_dl = torch.utils.data.DataLoader(val_raw_dataset, shuffle=True, batch_size=batch_size)\n",
    "# CHOOSE YOUR MODEL AND SAVE DIRECTORY\n",
    "model = models.resnet18(pretrained=False, num_classes=200).to(device)\n",
    "save_dir = \"resnet18_ckpts\"\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "382c3958-1ce5-4e3c-9ed0-1117bcb70d6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress: 100%|█████████▉| 234/235 [00:31<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, loss=defaultdict(<class 'int'>, {'loss': 4.54694629121334, 'top_1_acc': 0.07383089541120733, 'top_5_acc': 0.21560837765957447, 'top_10_acc': 0.31831781914893614}), eval_loss=defaultdict(<class 'int'>, {'loss': 4.722347981879052, 'top_1_acc': 0.06657247340425532, 'top_5_acc': 0.20848847519844135, 'top_10_acc': 0.31912677313419097})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:  23%|██▎       | 54/235 [00:07<00:24,  7.39it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_679/3223398177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_cb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch={epoch}, loss={loss}, eval_loss={eval_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"alexnet_ckpts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_679/3000824069.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optim, epochs, train_dl, val_dl, start_epoch, save_freq, save_best, save_dir, prev_best_loss, verbose, nb, train_cb)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                     \u001b[0mavg_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0moptimizer_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_cb(epoch, loss, eval_loss,model):\n",
    "    print(f\"Epoch={epoch}, loss={loss}, eval_loss={eval_loss}\")\n",
    "train(model=model, optim=optim, epochs=10, train_dl=train_dl, val_dl=train_dl, train_cb=train_cb, save_dir=save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
