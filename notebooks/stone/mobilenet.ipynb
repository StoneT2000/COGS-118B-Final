{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4baa0cd-01bb-4961-8778-f05c8099a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77877024-1c87-44f7-ac17-5396b1adaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd43bcb-16f3-403d-bb02-0298b56094f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 3, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"] / 255\n",
    "dataset[\"train\"] = dataset[\"train\"].permute(0,3,1,2)\n",
    "dataset[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10432ade-5b71-47c1-b13a-7a8a6644d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../data/tiny-imagenet-200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1b9939-0cc5-41b6-9029-a6ac1002b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv(f\"{data_path}/words.txt\", sep=\"\\t\", names=[\"id\", \"entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fde11f-c6cd-4ea4-9999-ce7307b0de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = {}\n",
    "for _, row in classes.iterrows():\n",
    "    id_to_label[row['id']] = row['entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6120daf6-17fb-48e9-848e-69e91a066f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## one hot encode labels and build dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7591c9f-0a58-4a6a-b98c-49f402665c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20682342-7000-4d1d-bb77-0cea02d81ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {}\n",
    "for i, label in enumerate(np.unique(labels)):\n",
    "    label_to_idx[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad6a7f5-b5a2-4faa-a0d4-78a5977c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    labels[i] = label_to_idx[labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8901d1b0-6afc-4276-9fc4-541d04bed52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.astype(int)\n",
    "labels = torch.from_numpy(labels)\n",
    "labels_onehot = torch.nn.functional.one_hot(labels,).float()\n",
    "raw_dataset = []\n",
    "for i in range(len(labels_onehot)):\n",
    "    raw_dataset.append((dataset[\"train\"][i], labels_onehot[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ac2a879d-b97e-4cec-b595-9ce00471aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accs(preds, labels):\n",
    "    k = 10\n",
    "    _, ind = torch.topk(preds, k)\n",
    "    \n",
    "    accs = {}\n",
    "    for _k in [1, 5, 10]:\n",
    "        acc = ind[:,:_k].eq(labels).any(1).sum() / len(labels)\n",
    "        accs[f\"top_{_k}_acc\"] = acc\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8d43ec6c-2e38-4484-bd05-8c5ec62ed6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_nb\n",
    "def evaluate(model, val_dl: DataLoader, verbose=True, nb=False):\n",
    "    model.eval()\n",
    "    _tqdm = tqdm_nb if nb else tqdm\n",
    "    if verbose:\n",
    "        pbar = _tqdm(enumerate(val_dl), total=len(val_dl), position=0, leave=True)\n",
    "        pbar.set_description(\"Evaluation Progress\")\n",
    "    with torch.no_grad():\n",
    "        batch_size = val_dl.batch_size\n",
    "        avg_loss = defaultdict(int)\n",
    "        num_train_imgs = len(val_dl.dataset)\n",
    "        c = 0\n",
    "        for batch_idx, data in enumerate(val_dl):\n",
    "            c += 1\n",
    "\n",
    "            imgs = data[0]\n",
    "            labels = data[1]\n",
    "            preds = model(imgs)\n",
    "            preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "\n",
    "            loss = {\n",
    "                \"loss\": loss_fnc(preds, labels),\n",
    "                # \"acc\": 0\n",
    "            }\n",
    "            for k, v in compute_accs(preds, labels.argmax(1).view(-1,1)).items():\n",
    "                loss[k] = v\n",
    "            for k, v in loss.items():\n",
    "                avg_loss[k] += v.item()\n",
    "            if verbose:\n",
    "                pbar.update()\n",
    "        for k in avg_loss.keys():\n",
    "            avg_loss[k] /= c\n",
    "    model.train()\n",
    "    if verbose:\n",
    "        pbar.close()\n",
    "    return avg_loss\n",
    "def train(\n",
    "    model,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    train_dl: DataLoader,\n",
    "    val_dl: DataLoader,\n",
    "    start_epoch=0,\n",
    "    save_freq=10,\n",
    "    save_best=True,\n",
    "    save_dir=\"./\",\n",
    "    prev_best_loss=np.inf,\n",
    "    verbose=True,\n",
    "    nb=False,\n",
    "    train_cb=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    train the vae model\n",
    "    \"\"\"\n",
    "    loss_fnc = torch.nn.CrossEntropyLoss()\n",
    "    _tqdm = tqdm_nb if nb else tqdm\n",
    "    model.train()\n",
    "    optimizer_idx = 0\n",
    "    num_train_imgs = len(train_dl.dataset)\n",
    "    batch_size = train_dl.batch_size\n",
    "    if verbose:\n",
    "        epoch_pbar = _tqdm(range(epochs), position=0, leave=True)\n",
    "        epoch_pbar.set_description(\"Progress\")\n",
    "        pbar = _tqdm(enumerate(train_dl), total=len(train_dl), position=0, leave=True)\n",
    "        pbar.set_description(\"Current Epoch Progress\")\n",
    "        epoch_pbar.update(start_epoch)\n",
    "    for epoch in range(start_epoch, start_epoch + epochs):\n",
    "        avg_loss = defaultdict(int)\n",
    "        c = 0\n",
    "        if verbose:\n",
    "            epoch_pbar.update()\n",
    "            pbar.reset()\n",
    "        for batch_idx, data in enumerate(train_dl):\n",
    "\n",
    "            c += 1\n",
    "            \n",
    "            imgs = data[0]\n",
    "            labels = data[1]\n",
    "            # imgs (B, C=1, W, H)\n",
    "            optim.zero_grad()\n",
    "            preds = model(imgs)\n",
    "            preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "            loss = {\n",
    "                \"loss\": loss_fnc(preds, labels),\n",
    "                # \"acc\": \n",
    "            }\n",
    "            for k, v in compute_accs(preds, labels.argmax(1).view(-1,1)).items():\n",
    "                loss[k] = v\n",
    "            for k, v in loss.items():\n",
    "                avg_loss[k] += v.item()\n",
    "            optimizer_idx += 1\n",
    "            loss[\"loss\"].backward()\n",
    "            optim.step()\n",
    "            if verbose:\n",
    "                pbar.update()\n",
    "        for k in avg_loss.keys():\n",
    "            avg_loss[k] /= c\n",
    "        eval_loss = evaluate(model, val_dl, nb=nb, verbose=False)\n",
    "        if train_cb:\n",
    "            train_cb(epoch=epoch, loss=avg_loss, eval_loss=eval_loss, model=model)\n",
    "        save = False\n",
    "        if eval_loss[\"loss\"] < prev_best_loss:\n",
    "            prev_best_loss = eval_loss[\"loss\"]\n",
    "            if save_best:\n",
    "                save = True\n",
    "        if epoch % save_freq == 0 or epoch == epochs - 1:\n",
    "            save = True\n",
    "        if save:\n",
    "            torch.save(\n",
    "                dict(\n",
    "                    model_state_dict=model.state_dict(),\n",
    "                    optim_State_dict=optim.state_dict(),\n",
    "                    epoch=epoch,\n",
    "                    loss=avg_loss,\n",
    "                    eval_loss=eval_loss,\n",
    "                    prev_best_loss=prev_best_loss,\n",
    "                ),\n",
    "                osp.join(save_dir, f\"ckpt_{epoch}.pt\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b9c12240-e1d1-4506-86ce-99d4c91a2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "raw_dataset = shuffle(raw_dataset, random_state=3407)\n",
    "train_raw_dataset = raw_dataset[:60000]\n",
    "val_raw_dataset = raw_dataset[60000:60000+20000]\n",
    "test_raw_dataset = raw_dataset[60000+20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e7ea674a-03cd-4156-94cf-755e443c6a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8531c09c-0fa0-4dec-8b8f-e51e4d20f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3407)\n",
    "batch_size=16\n",
    "train_dl = torch.utils.data.DataLoader(train_raw_dataset[:2000], shuffle=True, batch_size=batch_size)\n",
    "val_dl = torch.utils.data.DataLoader(val_raw_dataset[:2000], shuffle=True, batch_size=batch_size)\n",
    "mobilenet_v2 = models.mobilenet_v2(pretrained=False, num_classes=200)\n",
    "optim = torch.optim.Adam(mobilenet_v2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "382c3958-1ce5-4e3c-9ed0-1117bcb70d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:   0%|                                      | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, loss=defaultdict(<class 'int'>, {'loss': 5.29936605834961, 'top_1_acc': 0.004, 'top_5_acc': 0.0205, 'top_10_acc': 0.0475}), eval_loss=defaultdict(<class 'int'>, {'loss': 5.295231540679931, 'top_1_acc': 0.012, 'top_5_acc': 0.034, 'top_10_acc': 0.0745})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:   2%|▍                             | 2/125 [00:00<00:09, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1, loss=defaultdict(<class 'int'>, {'loss': 5.2977480163574215, 'top_1_acc': 0.0055, 'top_5_acc': 0.0285, 'top_10_acc': 0.063}), eval_loss=defaultdict(<class 'int'>, {'loss': 5.294805473327637, 'top_1_acc': 0.013, 'top_5_acc': 0.0485, 'top_10_acc': 0.0825})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:   2%|▍                             | 2/125 [00:00<00:09, 13.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=2, loss=defaultdict(<class 'int'>, {'loss': 5.298720592498779, 'top_1_acc': 0.006, 'top_5_acc': 0.0405, 'top_10_acc': 0.073}), eval_loss=defaultdict(<class 'int'>, {'loss': 5.300784381866455, 'top_1_acc': 0.005, 'top_5_acc': 0.038, 'top_10_acc': 0.0785})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:   2%|▍                             | 2/125 [00:00<00:09, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=3, loss=defaultdict(<class 'int'>, {'loss': 5.296588264465332, 'top_1_acc': 0.011, 'top_5_acc': 0.042, 'top_10_acc': 0.0755}), eval_loss=defaultdict(<class 'int'>, {'loss': 5.297063140869141, 'top_1_acc': 0.009, 'top_5_acc': 0.0385, 'top_10_acc': 0.0795})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:   2%|▍                             | 2/125 [00:00<00:09, 13.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=4, loss=defaultdict(<class 'int'>, {'loss': 5.294265602111817, 'top_1_acc': 0.0125, 'top_5_acc': 0.0395, 'top_10_acc': 0.075}), eval_loss=defaultdict(<class 'int'>, {'loss': 5.29630114364624, 'top_1_acc': 0.01, 'top_5_acc': 0.045, 'top_10_acc': 0.0775})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:   2%|▍                             | 2/125 [00:00<00:09, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=5, loss=defaultdict(<class 'int'>, {'loss': 5.294894966125488, 'top_1_acc': 0.012, 'top_5_acc': 0.0385, 'top_10_acc': 0.0715}), eval_loss=defaultdict(<class 'int'>, {'loss': 5.300703346252441, 'top_1_acc': 0.006, 'top_5_acc': 0.038, 'top_10_acc': 0.073})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:   2%|▍                             | 2/125 [00:00<00:09, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=6, loss=defaultdict(<class 'int'>, {'loss': 5.293234920501709, 'top_1_acc': 0.0135, 'top_5_acc': 0.0425, 'top_10_acc': 0.074}), eval_loss=defaultdict(<class 'int'>, {'loss': 5.296468486785889, 'top_1_acc': 0.009, 'top_5_acc': 0.04, 'top_10_acc': 0.072})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:   2%|▍                             | 2/125 [00:00<00:09, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=7, loss=defaultdict(<class 'int'>, {'loss': 5.300375061035156, 'top_1_acc': 0.0065, 'top_5_acc': 0.036, 'top_10_acc': 0.0665}), eval_loss=defaultdict(<class 'int'>, {'loss': 5.298682540893554, 'top_1_acc': 0.0075, 'top_5_acc': 0.035, 'top_10_acc': 0.066})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Epoch Progress:  70%|████████████████████▍        | 88/125 [00:06<00:03, 12.13it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n9/4grjffpn5wd1jpjknw1c79_80000gn/T/ipykernel_35096/3174302453.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_cb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch={epoch}, loss={loss}, eval_loss={eval_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmobilenet_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/n9/4grjffpn5wd1jpjknw1c79_80000gn/T/ipykernel_35096/1393899396.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optim, epochs, train_dl, val_dl, start_epoch, save_freq, save_best, save_dir, prev_best_loss, verbose, nb, train_cb)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# imgs (B, C=1, W, H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             loss = {\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torchvision/models/mobilenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torchvision/models/mobilenetv2.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Cannot use \"squeeze\" as batch-size can be 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torchvision/models/mobilenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/cogs118b/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2280\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2282\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2283\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_cb(epoch, loss, eval_loss,model):\n",
    "    print(f\"Epoch={epoch}, loss={loss}, eval_loss={eval_loss}\")\n",
    "train(model=mobilenet_v2, optim=optim, epochs=10, train_dl=train_dl, val_dl=train_dl, train_cb=train_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "31d2bc53-92d3-4d0e-8840-71b72d08462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([148,   8,   2,   3,   4])\n",
      "tensor(186)\n",
      "torch.Size([16, 5]) torch.Size([16])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/4grjffpn5wd1jpjknw1c79_80000gn/T/ipykernel_35096/3907569554.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a = torch.nn.functional.softmax(a)\n"
     ]
    }
   ],
   "source": [
    "for p in train_dl:\n",
    "    a = mobilenet_v2(p[0])\n",
    "    a = torch.nn.functional.softmax(a, dim=1)\n",
    "    # k=5\n",
    "    # ind = np.argpartition(a, -k, 1)[-k:]\n",
    "    # # print(a.argmax(1), p[1].argmax(1))\n",
    "    # print(ind.shape)\n",
    "    k=5\n",
    "    v, ind = torch.topk(a, k)\n",
    "    print(ind[0])\n",
    "    # print(p[1].argmax(1))\n",
    "    print(p[1][0].argmax())\n",
    "    print(ind.shape, p[1].argmax(1).shape)\n",
    "    print(ind.eq(p[1].argmax(1).view(-1, 1)).any(1))\n",
    "    \n",
    "    # print(ind, a[0][ind])\n",
    "    # print(a[0][ind[:,0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c0a3d-a0df-49b8-a642-51998e67934b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
